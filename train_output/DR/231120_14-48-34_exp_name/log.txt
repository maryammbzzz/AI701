[37m[36mINFO[0m[0m 11/20 14:48:34 | Command :: train_all.py exp_name --dataset DR --data_dir /home/maryam.arjemandi/Original_Data/ --algorithm MIRO
Environment:
	Python: 3.8.18
	PyTorch: 1.7.1
	Torchvision: 0.8.2
	CUDA: 10.2
	CUDNN: 7605
	NumPy: 1.21.4
	PIL: 9.0.1
Args:
	algorithm: MIRO
	checkpoint_freq: None
	configs: []
	data_dir: /home/maryam.arjemandi/Original_Data
	dataset: DR
	debug: False
	deterministic: True
	evalmode: fast
	holdout_fraction: 0.2
	model_save: None
	name: exp_name
	out_dir: train_output/DR/231120_14-48-34_exp_name
	out_root: train_output/DR
	prebuild_loader: False
	seed: 0
	show: False
	steps: None
	tb_freq: 10
	test_envs: None
	trial_seed: 0
	unique_name: 231120_14-48-34_exp_name
	work_dir: .
HParams:
	data_augmentation: True
	val_augment: False
	resnet18: False
	resnet_dropout: 0.0
	class_balanced: False
	optimizer: adam
	freeze_bn: True
	pretrained: True
	lr: 5e-05
	batch_size: 16
	weight_decay: 0.0
	swad: True
	swad_kwargs: 
	  n_converge: 3
	  n_tolerance: 6
	  tolerance_ratio: 0.3
	test_batchsize: 128
	model: swag_regnety_16gf
	feat_layers: stem_block
	ld: 0.1
	lr_mult: 10.0
Dataset:
	[DR] #envs=3, #classes=5
	env0: aptos (#3662)
	env1: eyepacs (#35126)
	env2: messidor (#1002)

[37m[36mINFO[0m[0m 11/20 14:48:34 | n_steps = 5001
[37m[36mINFO[0m[0m 11/20 14:48:34 | checkpoint_freq = 200
[37m[36mINFO[0m[0m 11/20 14:48:34 | n_steps is updated to 5001 => 5001 for checkpointing
[37m[36mINFO[0m[0m 11/20 14:48:34 | Target test envs = [[0], [1], [2]]
[37m[36mINFO[0m[0m 11/20 14:48:34 | 
[37m[36mINFO[0m[0m 11/20 14:48:34 | Testenv name escaping te_aptos -> te_aptos
[37m[36mINFO[0m[0m 11/20 14:48:34 | Test envs = [0], name = te_aptos
[37m[36mINFO[0m[0m 11/20 14:48:34 | Batch sizes for each domain: [0, 16, 16] (total=32)
[37m[36mINFO[0m[0m 11/20 14:48:34 | steps-per-epoch for each domain: 1756.31, 50.12 -> min = 50.12
[37m[36mINFO[0m[0m 11/20 14:48:46 | # of params = 161150365
[37m[36mINFO[0m[0m 11/20 14:58:56 | test_in     test_out    train_in    train_out   tr_outloss  env0_in     env0_out    env1_out    env2_out    step        epoch       loss        reg_loss    step_time   eval_time  
[37m[36mINFO[0m[0m 11/20 14:58:56 | 0.261433    0.282787    0.000000    0.172571    2.509164    0.261433    0.282787    0.160142    0.185000    0           0.000000    2.051111    -5.756461   0.629836    608.906200 
[37m[36mINFO[0m[0m 11/20 15:10:29 | 0.550853    0.538251    0.000000    0.694244    0.928364    0.550853    0.538251    0.743488    0.645000    200         3.990025    4.031214    28.676076   1.043558    474.855692 
[37m[36mINFO[0m[0m 11/20 15:23:06 | 0.536519    0.530055    0.000000    0.694813    0.874153    0.536519    0.530055    0.744626    0.645000    400         7.980050    2.090082    11.185207   1.017979    543.514797 
[37m[36mINFO[0m[0m 11/20 15:35:01 | 0.549488    0.557377    0.000000    0.665560    0.888067    0.549488    0.557377    0.726121    0.605000    600         11.970075   1.846505    8.774129    1.010546    503.153918 
[37m[36mINFO[0m[0m 11/20 15:46:15 | 0.550853    0.539617    0.000000    0.694093    0.904775    0.550853    0.539617    0.748185    0.640000    800         15.960100   1.653349    7.494856    1.086885    448.203806 
[37m[36mINFO[0m[0m 11/20 15:57:42 | 0.553584    0.543716    0.000000    0.699520    0.891737    0.553584    0.543716    0.749039    0.650000    1000        19.950125   1.536751    6.432381    1.068439    462.918860 
[37m[36mINFO[0m[0m 11/20 16:08:45 | 0.662116    0.653005    0.000000    0.732349    0.771477    0.662116    0.653005    0.764698    0.700000    1200        23.940150   1.414956    5.389372    1.028985    447.634727 
[37m[36mINFO[0m[0m 11/20 16:19:44 | 0.363823    0.360656    0.000000    0.625080    0.982831    0.363823    0.360656    0.715160    0.535000    1400        27.930175   1.423863    5.345078    0.995376    450.312383 
[37m[36mINFO[0m[0m 11/20 16:31:27 | 0.679522    0.666667    0.000000    0.691922    0.893730    0.679522    0.666667    0.763843    0.620000    1600        31.920200   1.282907    4.464017    1.038822    482.227841 
[37m[36mINFO[0m[0m 11/20 16:43:05 | 0.695563    0.699454    0.000000    0.710418    0.800820    0.695563    0.699454    0.765836    0.655000    1800        35.910224   1.293976    4.188801    1.031348    481.602875 
[37m[36mINFO[0m[0m 11/20 16:54:56 | 0.683959    0.646175    0.000000    0.705107    0.828664    0.683959    0.646175    0.780214    0.630000    2000        39.900249   1.180388    3.346250    1.037670    492.332536 
[37m[36mINFO[0m[0m 11/20 17:05:51 | 0.580205    0.558743    0.000000    0.712989    0.793769    0.580205    0.558743    0.765979    0.660000    2200        43.890274   1.116657    2.805484    1.012000    441.987059 
[37m[36mINFO[0m[0m 11/20 17:17:02 | 0.583959    0.571038    0.000000    0.728319    0.822341    0.583959    0.571038    0.781637    0.675000    2400        47.880299   1.067176    2.518512    1.037889    452.878002 
[37m[36mINFO[0m[0m 11/20 17:27:55 | 0.716382    0.719945    0.000000    0.724546    0.824637    0.716382    0.719945    0.774093    0.675000    2600        51.870324   0.954889    1.587495    1.024281    437.999848 
[37m[36mINFO[0m[0m 11/20 17:39:06 | 0.660068    0.668033    0.000000    0.732233    0.744040    0.660068    0.668033    0.789466    0.675000    2800        55.860349   0.990768    1.716188    1.001431    458.158645 
[37m[36mINFO[0m[0m 11/20 17:49:48 | 0.556655    0.550546    0.000000    0.712242    0.807453    0.556655    0.550546    0.784484    0.640000    3000        59.850374   0.951943    1.379193    1.010682    428.223011 
[37m[36mINFO[0m[0m 11/20 18:00:34 | 0.500341    0.500000    0.000000    0.698754    0.799481    0.500341    0.500000    0.777509    0.620000    3200        63.840399   0.868647    0.791635    0.994427    434.295624 
[37m[36mINFO[0m[0m 11/20 18:12:35 | 0.614676    0.622951    0.000000    0.728007    0.739208    0.614676    0.622951    0.796014    0.660000    3400        67.830424   0.898114    0.883602    1.010904    504.637622 
[37m[36mINFO[0m[0m 11/20 18:24:21 | 0.554608    0.549180    0.000000    0.696566    0.888327    0.554608    0.549180    0.763132    0.630000    3600        71.820449   0.848951    0.635254    1.078546    476.303031 
[37m[36mINFO[0m[0m 11/20 18:36:44 | 0.525597    0.551913    0.000000    0.648728    0.921063    0.525597    0.551913    0.712456    0.585000    3800        75.810474   0.798082    0.234127    1.098695    509.157558 
[37m[36mINFO[0m[0m 11/20 18:49:32 | 0.675085    0.674863    0.000000    0.740934    0.736391    0.675085    0.674863    0.796868    0.685000    4000        79.800499   0.739978    -0.141583   1.099597    530.455304 
[37m[36mINFO[0m[0m 11/20 19:01:56 | 0.602389    0.599727    0.000000    0.739582    0.764865    0.602389    0.599727    0.794164    0.685000    4200        83.790524   0.715444    -0.271580   1.125828    503.946554 
[37m[36mINFO[0m[0m 11/20 19:13:24 | 0.575085    0.560109    0.000000    0.661530    0.846420    0.575085    0.560109    0.743060    0.580000    4400        87.780549   0.696724    -0.440898   1.054308    460.715525 
[37m[36mINFO[0m[0m 11/20 19:24:25 | 0.606826    0.583333    0.000000    0.748888    0.744052    0.606826    0.583333    0.782776    0.715000    4600        91.770574   0.688403    -0.474348   1.010090    446.387357 
[37m[36mINFO[0m[0m 11/20 19:35:12 | 0.716382    0.715847    0.000000    0.728532    0.768831    0.716382    0.715847    0.782064    0.675000    4800        95.760599   0.614161    -0.897315   1.006541    433.612944 
[37m[36mINFO[0m[0m 11/20 19:46:27 | 0.722184    0.722678    0.000000    0.681388    0.905672    0.722184    0.722678    0.782776    0.580000    5000        99.750623   0.637557    -0.911824   1.016996    459.499172 
[37m[36mINFO[0m[0m 11/20 19:46:30 | ---
[37m[33mWARNING[0m[0m 11/20 19:46:33 | Evaluate SWAD ...
[37m[36mINFO[0m[0m 11/20 19:54:08 | 0.645734    0.658470    0.000000    0.735240    0.729502    0.645734    0.658470    0.785480    0.685000    [201-5000]  (N=26)
[37m[36mINFO[0m[0m 11/20 19:54:08 | test-domain validation = 72.218%
[37m[36mINFO[0m[0m 11/20 19:54:08 | training-domain validation = 60.683%
[37m[36mINFO[0m[0m 11/20 19:54:08 | SWAD = 64.573%
[37m[36mINFO[0m[0m 11/20 19:54:08 | SWAD (inD) = 73.524%
[37m[36mINFO[0m[0m 11/20 19:54:08 | 
[37m[36mINFO[0m[0m 11/20 19:54:09 | Testenv name escaping te_eyepacs -> te_eyepacs
[37m[36mINFO[0m[0m 11/20 19:54:09 | Test envs = [1], name = te_eyepacs
[37m[36mINFO[0m[0m 11/20 19:54:09 | Batch sizes for each domain: [16, 0, 16, 16] (total=48)
[37m[36mINFO[0m[0m 11/20 19:54:09 | steps-per-epoch for each domain: 183.12, 50.12, 52.88 -> min = 50.12
[37m[36mINFO[0m[0m 11/20 19:54:32 | # of params = 161150365
[37m[36mINFO[0m[0m 11/20 20:17:23 | test_in     test_out    train_in    train_out   tr_outloss  env0_out    env1_in     env1_out    env2_out    env3_out    step        epoch       loss        reg_loss    step_time   eval_time  
[37m[36mINFO[0m[0m 11/20 20:17:23 | 0.705669    0.703915    0.000000    0.452519    2.784077    0.484973    0.705669    0.703915    0.465000    0.407583    0           0.000000    2.642934    -5.756460   0.947516    1369.16805 
